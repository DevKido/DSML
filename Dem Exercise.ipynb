{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to widen the notebook screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#notebook { padding-top:0px !important; } .container { width:100% !important; } .end_space { min-height:0px !important; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\n",
    "    '<style>'\n",
    "        '#notebook { padding-top:0px !important; } ' \n",
    "        '.container { width:100% !important; } '\n",
    "        '.end_space { min-height:0px !important; } '\n",
    "    '</style>'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Self code: to run pyspark here\n",
    "    findspark\n",
    "    init()\n",
    "    SparkConf\n",
    "    SparkContext\n",
    "    SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Softwares\\\\Spark\\\\spark-2.4.7-bin-hadoop2.7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('appEx').setMaster('local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf= conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession(sc).builder.appName('sessionEx').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./Resources/Python-and-Spark-for-Big-Data-master/Spark_DataFrame_Project_Exercise/walmart_stock.csv',\n",
    "                   inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "|               Date|              Open|              High|               Low|             Close|  Volume|         Adj Close|\n",
      "+-------------------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "|2012-01-03 00:00:00|         59.970001|         61.060001|         59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04 00:00:00|60.209998999999996|         60.349998|         59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05 00:00:00|         59.349998|         59.619999|         58.369999|         59.419998|12768200|         51.825539|\n",
      "|2012-01-06 00:00:00|         59.419998|         59.450001|         58.869999|              59.0| 8069400|          51.45922|\n",
      "|2012-01-09 00:00:00|         59.029999|         59.549999|         58.919998|             59.18| 6679300|51.616215000000004|\n",
      "|2012-01-10 00:00:00|             59.43|59.709998999999996|             58.98|59.040001000000004| 6907300|         51.494109|\n",
      "|2012-01-11 00:00:00|         59.060001|         59.529999|59.040001000000004|         59.400002| 6365600|         51.808098|\n",
      "|2012-01-12 00:00:00|59.790001000000004|              60.0|         59.400002|              59.5| 7236400|51.895315999999994|\n",
      "|2012-01-13 00:00:00|             59.18|59.610001000000004|59.009997999999996|59.540001000000004| 7729300|51.930203999999996|\n",
      "|2012-01-17 00:00:00|         59.869999|60.110001000000004|             59.52|         59.849998| 8500000|         52.200581|\n",
      "|2012-01-18 00:00:00|59.790001000000004|         60.029999|         59.650002|60.009997999999996| 5911400|         52.340131|\n",
      "|2012-01-19 00:00:00|             59.93|             60.73|             59.75|60.610001000000004| 9234600|         52.863447|\n",
      "|2012-01-20 00:00:00|             60.75|             61.25|         60.669998|61.009997999999996|10378800|53.212320999999996|\n",
      "|2012-01-23 00:00:00|         60.810001|             60.98|60.509997999999996|             60.91| 7134100|         53.125104|\n",
      "|2012-01-24 00:00:00|             60.75|              62.0|             60.75|61.389998999999996| 7362800| 53.54375400000001|\n",
      "|2012-01-25 00:00:00|             61.18|61.610001000000004|61.040001000000004|         61.470001| 5915800| 53.61353100000001|\n",
      "|2012-01-26 00:00:00|         61.799999|             61.84|             60.77|         60.970001| 7436200|         53.177436|\n",
      "|2012-01-27 00:00:00|60.860001000000004|         61.119999|60.540001000000004|60.709998999999996| 6287300|         52.950665|\n",
      "|2012-01-30 00:00:00|         60.470001|             61.32|         60.349998|         61.299999| 7636900|53.465256999999994|\n",
      "|2012-01-31 00:00:00|         61.529999|             61.57|         60.580002|61.360001000000004| 9761500|53.517590000000006|\n",
      "+-------------------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2012, 1, 3, 0, 0), Open=59.970001, High=61.060001, Low=59.869999, Close=60.330002, Volume=12668800, Adj Close=52.619234999999996),\n",
       " Row(Date=datetime.datetime(2012, 1, 4, 0, 0), Open=60.209998999999996, High=60.349998, Low=59.470001, Close=59.709998999999996, Volume=9593300, Adj Close=52.078475),\n",
       " Row(Date=datetime.datetime(2012, 1, 5, 0, 0), Open=59.349998, High=59.619999, Low=58.369999, Close=59.419998, Volume=12768200, Adj Close=51.825539),\n",
       " Row(Date=datetime.datetime(2012, 1, 6, 0, 0), Open=59.419998, High=59.450001, Low=58.869999, Close=59.0, Volume=8069400, Adj Close=51.45922),\n",
       " Row(Date=datetime.datetime(2012, 1, 9, 0, 0), Open=59.029999, High=59.549999, Low=58.919998, Close=59.18, Volume=6679300, Adj Close=51.616215000000004)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
      "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
      "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
      "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
      "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType,DecimalType,DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = StructType([\n",
    "                        StructField('Date',     DateType()),\n",
    "                        StructField('Open',     DecimalType(10,2)),\n",
    "                        StructField('High',     DecimalType(10,2)),\n",
    "                        StructField('Low' ,     DecimalType(10,2)),\n",
    "                        StructField('Close',    DecimalType(10,2)),\n",
    "                        StructField('Volume',   DecimalType(10,2)),\n",
    "                        StructField('Adj Close',DecimalType(10,2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./Resources/Python-and-Spark-for-Big-Data-master/Spark_DataFrame_Project_Exercise/walmart_stock.csv', schema=data_schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: decimal(10,2) (nullable = true)\n",
      " |-- High: decimal(10,2) (nullable = true)\n",
      " |-- Low: decimal(10,2) (nullable = true)\n",
      " |-- Close: decimal(10,2) (nullable = true)\n",
      " |-- Volume: decimal(10,2) (nullable = true)\n",
      " |-- Adj Close: decimal(10,2) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+-----------+---------+\n",
      "|      Date| Open| High|  Low|Close|     Volume|Adj Close|\n",
      "+----------+-----+-----+-----+-----+-----------+---------+\n",
      "|2012-01-03|59.97|61.06|59.87|60.33|12668800.00|    52.62|\n",
      "|2012-01-04|60.21|60.35|59.47|59.71| 9593300.00|    52.08|\n",
      "|2012-01-05|59.35|59.62|58.37|59.42|12768200.00|    51.83|\n",
      "|2012-01-06|59.42|59.45|58.87|59.00| 8069400.00|    51.46|\n",
      "|2012-01-09|59.03|59.55|58.92|59.18| 6679300.00|    51.62|\n",
      "|2012-01-10|59.43|59.71|58.98|59.04| 6907300.00|    51.49|\n",
      "|2012-01-11|59.06|59.53|59.04|59.40| 6365600.00|    51.81|\n",
      "|2012-01-12|59.79|60.00|59.40|59.50| 7236400.00|    51.90|\n",
      "|2012-01-13|59.18|59.61|59.01|59.54| 7729300.00|    51.93|\n",
      "|2012-01-17|59.87|60.11|59.52|59.85| 8500000.00|    52.20|\n",
      "|2012-01-18|59.79|60.03|59.65|60.01| 5911400.00|    52.34|\n",
      "|2012-01-19|59.93|60.73|59.75|60.61| 9234600.00|    52.86|\n",
      "|2012-01-20|60.75|61.25|60.67|61.01|10378800.00|    53.21|\n",
      "|2012-01-23|60.81|60.98|60.51|60.91| 7134100.00|    53.13|\n",
      "|2012-01-24|60.75|62.00|60.75|61.39| 7362800.00|    53.54|\n",
      "|2012-01-25|61.18|61.61|61.04|61.47| 5915800.00|    53.61|\n",
      "|2012-01-26|61.80|61.84|60.77|60.97| 7436200.00|    53.18|\n",
      "|2012-01-27|60.86|61.12|60.54|60.71| 6287300.00|    52.95|\n",
      "|2012-01-30|60.47|61.32|60.35|61.30| 7636900.00|    53.47|\n",
      "|2012-01-31|61.53|61.57|60.58|61.36| 9761500.00|    53.52|\n",
      "+----------+-----+-----+-----+-----+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df.withColumn('HV Ratio', df['High']/df['Volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+-----------+---------+---------------+\n",
      "|      Date| Open| High|  Low|Close|     Volume|Adj Close|       HV Ratio|\n",
      "+----------+-----+-----+-----+-----+-----------+---------+---------------+\n",
      "|2012-01-03|59.97|61.06|59.87|60.33|12668800.00|    52.62|0.0000048197146|\n",
      "|2012-01-04|60.21|60.35|59.47|59.71| 9593300.00|    52.08|0.0000062908488|\n",
      "|2012-01-05|59.35|59.62|58.37|59.42|12768200.00|    51.83|0.0000046694131|\n",
      "|2012-01-06|59.42|59.45|58.87|59.00| 8069400.00|    51.46|0.0000073673383|\n",
      "|2012-01-09|59.03|59.55|58.92|59.18| 6679300.00|    51.62|0.0000089156049|\n",
      "|2012-01-10|59.43|59.71|58.98|59.04| 6907300.00|    51.49|0.0000086444776|\n",
      "|2012-01-11|59.06|59.53|59.04|59.40| 6365600.00|    51.81|0.0000093518286|\n",
      "|2012-01-12|59.79|60.00|59.40|59.50| 7236400.00|    51.90|0.0000082914156|\n",
      "|2012-01-13|59.18|59.61|59.01|59.54| 7729300.00|    51.93|0.0000077122120|\n",
      "|2012-01-17|59.87|60.11|59.52|59.85| 8500000.00|    52.20|0.0000070717647|\n",
      "|2012-01-18|59.79|60.03|59.65|60.01| 5911400.00|    52.34|0.0000101549548|\n",
      "|2012-01-19|59.93|60.73|59.75|60.61| 9234600.00|    52.86|0.0000065763541|\n",
      "|2012-01-20|60.75|61.25|60.67|61.01|10378800.00|    53.21|0.0000059014530|\n",
      "|2012-01-23|60.81|60.98|60.51|60.91| 7134100.00|    53.13|0.0000085476795|\n",
      "|2012-01-24|60.75|62.00|60.75|61.39| 7362800.00|    53.54|0.0000084207095|\n",
      "|2012-01-25|61.18|61.61|61.04|61.47| 5915800.00|    53.61|0.0000104144832|\n",
      "|2012-01-26|61.80|61.84|60.77|60.97| 7436200.00|    53.18|0.0000083160754|\n",
      "|2012-01-27|60.86|61.12|60.54|60.71| 6287300.00|    52.95|0.0000097211840|\n",
      "|2012-01-30|60.47|61.32|60.35|61.30| 7636900.00|    53.47|0.0000080294360|\n",
      "|2012-01-31|61.53|61.57|60.58|61.36| 9761500.00|    53.52|0.0000063074323|\n",
      "+----------+-----+-----+-----+-----+-----------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Date|\n",
      "+----------+\n",
      "|2015-01-13|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "high_high = df_2.agg({'High':'max'})\n",
    "\n",
    "df_2.select('Date').filter(df_2['High'] == high_high.head()[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2015, 1, 13)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.select('Date').filter(df_2['High'] == df_2.agg({'High':'max'}).head()[0]).head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|avg(Close)|\n",
      "+----------+\n",
      "| 72.388450|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.agg({'Close':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|max(Volume)|min(Volume)|\n",
      "+-----------+-----------+\n",
      "|80898100.00| 2094900.00|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max,min\n",
    "df_2.select([max(df_2['Volume']), min(df_2['Volume'])]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.filter(df_2['Close'] < 60).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.141494435612083"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(  (df_2.filter(df_2['High'] > 80).count()) / (df_2.count()) ) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max,min,corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|  corr(High, Volume)|\n",
      "+--------------------+\n",
      "|-0.33843260582148915|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.select(corr(df_2['High'],df_2['Volume'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year,month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_2.withColumn('Year',year(df_2['Date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Year|max(High)|\n",
      "+----+---------+\n",
      "|2012|    77.60|\n",
      "|2013|    81.37|\n",
      "|2014|    88.09|\n",
      "|2015|    90.97|\n",
      "|2016|    75.19|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.groupBy('Year').max('High').orderBy('Year').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_2.withColumn('month', month(df_2['Date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|month|avg(Close)|\n",
      "+-----+----------+\n",
      "|    1| 71.448020|\n",
      "|    2| 71.306804|\n",
      "|    3| 71.777944|\n",
      "|    4| 72.973619|\n",
      "|    5| 72.309717|\n",
      "|    6| 72.495377|\n",
      "|    7| 74.439720|\n",
      "|    8| 73.029818|\n",
      "|    9| 72.184118|\n",
      "|   10| 71.578545|\n",
      "|   11| 72.111089|\n",
      "|   12| 72.847925|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.groupBy('month').mean('Close').orderBy('month').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
